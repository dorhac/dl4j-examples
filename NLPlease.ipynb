{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPlease.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dorhac/dl4j-examples/blob/master/NLPlease.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "RCV2plTfjwY2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "868de69a-eafc-4971-9e8b-3bb4c6523487"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/UnsupervisedMT.git\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'UnsupervisedMT'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 108 (delta 0), reused 0 (delta 0), pack-reused 105\u001b[K\n",
            "Receiving objects: 100% (108/108), 278.56 KiB | 3.67 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9qXw941NqgBs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d70ef8bb-5f6d-4bd5-ccf7-ad11973b95d5"
      },
      "cell_type": "code",
      "source": [
        "cd root/UnsupervisedMT/NMT\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "enfrB.sh           get_data_enfrA.sh  main.py         \u001b[0m\u001b[01;34msrc\u001b[0m/\n",
            "\u001b[01;32mget_data_deen.sh\u001b[0m*  \u001b[01;32mget_data_enfr.sh\u001b[0m*  \u001b[01;32mpreprocess.py\u001b[0m*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZqVgwoBuj83-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "6dcfe5f0-ec63-4a02-aab4-77ce09c74d54"
      },
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "chmod u+x ./get_data_enfrA.sh\n",
        "./get_data_enfrA.sh"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning Moses from GitHub repository...\n",
            "Cloning into 'mosesdecoder'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/97)   \u001b[K\rremote: Counting objects:   2% (2/97)   \u001b[K\rremote: Counting objects:   3% (3/97)   \u001b[K\rremote: Counting objects:   4% (4/97)   \u001b[K\rremote: Counting objects:   5% (5/97)   \u001b[K\rremote: Counting objects:   6% (6/97)   \u001b[K\rremote: Counting objects:   7% (7/97)   \u001b[K\rremote: Counting objects:   8% (8/97)   \u001b[K\rremote: Counting objects:   9% (9/97)   \u001b[K\rremote: Counting objects:  10% (10/97)   \u001b[K\rremote: Counting objects:  11% (11/97)   \u001b[K\rremote: Counting objects:  12% (12/97)   \u001b[K\rremote: Counting objects:  13% (13/97)   \u001b[K\rremote: Counting objects:  14% (14/97)   \u001b[K\rremote: Counting objects:  15% (15/97)   \u001b[K\rremote: Counting objects:  16% (16/97)   \u001b[K\rremote: Counting objects:  17% (17/97)   \u001b[K\rremote: Counting objects:  18% (18/97)   \u001b[K\rremote: Counting objects:  19% (19/97)   \u001b[K\rremote: Counting objects:  20% (20/97)   \u001b[K\rremote: Counting objects:  21% (21/97)   \u001b[K\rremote: Counting objects:  22% (22/97)   \u001b[K\rremote: Counting objects:  23% (23/97)   \u001b[K\rremote: Counting objects:  24% (24/97)   \u001b[K\rremote: Counting objects:  25% (25/97)   \u001b[K\rremote: Counting objects:  26% (26/97)   \u001b[K\rremote: Counting objects:  27% (27/97)   \u001b[K\rremote: Counting objects:  28% (28/97)   \u001b[K\rremote: Counting objects:  29% (29/97)   \u001b[K\rremote: Counting objects:  30% (30/97)   \u001b[K\rremote: Counting objects:  31% (31/97)   \u001b[K\rremote: Counting objects:  32% (32/97)   \u001b[K\rremote: Counting objects:  34% (33/97)   \u001b[K\rremote: Counting objects:  35% (34/97)   \u001b[K\rremote: Counting objects:  36% (35/97)   \u001b[K\rremote: Counting objects:  37% (36/97)   \u001b[K\rremote: Counting objects:  38% (37/97)   \u001b[K\rremote: Counting objects:  39% (38/97)   \u001b[K\rremote: Counting objects:  40% (39/97)   \u001b[K\rremote: Counting objects:  41% (40/97)   \u001b[K\rremote: Counting objects:  42% (41/97)   \u001b[K\rremote: Counting objects:  43% (42/97)   \u001b[K\rremote: Counting objects:  44% (43/97)   \u001b[K\rremote: Counting objects:  45% (44/97)   \u001b[K\rremote: Counting objects:  46% (45/97)   \u001b[K\rremote: Counting objects:  47% (46/97)   \u001b[K\rremote: Counting objects:  48% (47/97)   \u001b[K\rremote: Counting objects:  49% (48/97)   \u001b[K\rremote: Counting objects:  50% (49/97)   \u001b[K\rremote: Counting objects:  51% (50/97)   \u001b[K\rremote: Counting objects:  52% (51/97)   \u001b[K\rremote: Counting objects:  53% (52/97)   \u001b[K\rremote: Counting objects:  54% (53/97)   \u001b[K\rremote: Counting objects:  55% (54/97)   \u001b[K\rremote: Counting objects:  56% (55/97)   \u001b[K\rremote: Counting objects:  57% (56/97)   \u001b[K\rremote: Counting objects:  58% (57/97)   \u001b[K\rremote: Counting objects:  59% (58/97)   \u001b[K\rremote: Counting objects:  60% (59/97)   \u001b[K\rremote: Counting objects:  61% (60/97)   \u001b[K\rremote: Counting objects:  62% (61/97)   \u001b[K\rremote: Counting objects:  63% (62/97)   \u001b[K\rremote: Counting objects:  64% (63/97)   \u001b[K\rremote: Counting objects:  65% (64/97)   \u001b[K\rremote: Counting objects:  67% (65/97)   \u001b[K\rremote: Counting objects:  68% (66/97)   \u001b[K\rremote: Counting objects:  69% (67/97)   \u001b[K\rremote: Counting objects:  70% (68/97)   \u001b[K\rremote: Counting objects:  71% (69/97)   \u001b[K\rremote: Counting objects:  72% (70/97)   \u001b[K\rremote: Counting objects:  73% (71/97)   \u001b[K\rremote: Counting objects:  74% (72/97)   \u001b[K\rremote: Counting objects:  75% (73/97)   \u001b[K\rremote: Counting objects:  76% (74/97)   \u001b[K\rremote: Counting objects:  77% (75/97)   \u001b[K\rremote: Counting objects:  78% (76/97)   \u001b[K\rremote: Counting objects:  79% (77/97)   \u001b[K\rremote: Counting objects:  80% (78/97)   \u001b[K\rremote: Counting objects:  81% (79/97)   \u001b[K\rremote: Counting objects:  82% (80/97)   \u001b[K\rremote: Counting objects:  83% (81/97)   \u001b[K\rremote: Counting objects:  84% (82/97)   \u001b[K\rremote: Counting objects:  85% (83/97)   \u001b[K\rremote: Counting objects:  86% (84/97)   \u001b[K\rremote: Counting objects:  87% (85/97)   \u001b[K\rremote: Counting objects:  88% (86/97)   \u001b[K\rremote: Counting objects:  89% (87/97)   \u001b[K\rremote: Counting objects:  90% (88/97)   \u001b[K\rremote: Counting objects:  91% (89/97)   \u001b[K\rremote: Counting objects:  92% (90/97)   \u001b[K\rremote: Counting objects:  93% (91/97)   \u001b[K\rremote: Counting objects:  94% (92/97)   \u001b[K\rremote: Counting objects:  95% (93/97)   \u001b[K\rremote: Counting objects:  96% (94/97)   \u001b[K\rremote: Counting objects:  97% (95/97)   \u001b[K\rremote: Counting objects:  98% (96/97)   \u001b[K\rremote: Counting objects: 100% (97/97)   \u001b[K\rremote: Counting objects: 100% (97/97), done.\u001b[K\n",
            "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 147295 (delta 50), reused 69 (delta 40), pack-reused 147198\u001b[K\n",
            "Receiving objects: 100% (147295/147295), 129.73 MiB | 21.12 MiB/s, done.\n",
            "Resolving deltas: 100% (113815/113815), done.\n",
            "Moses found in: /root/UnsupervisedMT/NMT/tools/mosesdecoder\n",
            "Cloning fastBPE from GitHub repository...\n",
            "Cloning into 'fastBPE'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Total 24 (delta 0), reused 0 (delta 0), pack-reused 24\u001b[K\n",
            "Unpacking objects: 100% (24/24), done.\n",
            "fastBPE found in: /root/UnsupervisedMT/NMT/tools/fastBPE\n",
            "Compiling fastBPE...\n",
            "fastBPE compiled in: /root/UnsupervisedMT/NMT/tools/fastBPE/fast\n",
            "Cloning fastText from GitHub repository...\n",
            "Cloning into 'fastText'...\n",
            "remote: Enumerating objects: 84, done.\u001b[K\n",
            "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 3101 (delta 43), reused 66 (delta 43), pack-reused 3017\u001b[K\n",
            "Receiving objects: 100% (3101/3101), 7.81 MiB | 25.08 MiB/s, done.\n",
            "Resolving deltas: 100% (1942/1942), done.\n",
            "fastText found in: /root/UnsupervisedMT/NMT/tools/fastText\n",
            "Compiling fastText...\n",
            "c++ -pthread -std=c++0x -march=native -O3 -funroll-loops -c src/args.cc\n",
            "c++ -pthread -std=c++0x -march=native -O3 -funroll-loops -c src/matrix.cc\n",
            "c++ -pthread -std=c++0x -march=native -O3 -funroll-loops -c src/dictionary.cc\n",
            "c++ -pthread -std=c++0x -march=native -O3 -funroll-loops -c src/loss.cc\n",
            "c++ -pthread -std=c++0x -march=native -O3 -funroll-loops -c src/productquantizer.cc\n",
            "c++ -pthread -std=c++0x -march=native -O3 -funroll-loops -c src/densematrix.cc\n",
            "c++ -pthread -std=c++0x -march=native -O3 -funroll-loops -c src/quantmatrix.cc\n",
            "c++ -pthread -std=c++0x -march=native -O3 -funroll-loops -c src/vector.cc\n",
            "c++ -pthread -std=c++0x -march=native -O3 -funroll-loops -c src/model.cc\n",
            "c++ -pthread -std=c++0x -march=native -O3 -funroll-loops -c src/utils.cc\n",
            "c++ -pthread -std=c++0x -march=native -O3 -funroll-loops -c src/meter.cc\n",
            "c++ -pthread -std=c++0x -march=native -O3 -funroll-loops -c src/fasttext.cc\n",
            "\u001b[01m\u001b[Ksrc/fasttext.cc:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kvoid fasttext::FastText::quantize(const fasttext::Args&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Ksrc/fasttext.cc:323:45:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kstd::vector<int> fasttext::FastText::selectEmbeddings(int32_t) const\u001b[m\u001b[K’ is deprecated: selectEmbeddings is being deprecated. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     auto idx = selectEmbeddings(qargs.cutoff\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Ksrc/fasttext.cc:293:22:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " std::vector<int32_t> \u001b[01;36m\u001b[KFastText\u001b[m\u001b[K::selectEmbeddings(int32_t cutoff) const {\n",
            "                      \u001b[01;36m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Ksrc/fasttext.cc:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kvoid fasttext::FastText::lazyComputeWordVectors()\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Ksrc/fasttext.cc:551:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid fasttext::FastText::precomputeWordVectors(fasttext::DenseMatrix&)\u001b[m\u001b[K’ is deprecated: precomputeWordVectors is being deprecated. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     precomputeWordVectors(*wordVectors_\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Ksrc/fasttext.cc:534:6:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " void \u001b[01;36m\u001b[KFastText\u001b[m\u001b[K::precomputeWordVectors(DenseMatrix& wordVectors) {\n",
            "      \u001b[01;36m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "c++ -pthread -std=c++0x -march=native -O3 -funroll-loops args.o matrix.o dictionary.o loss.o productquantizer.o densematrix.o quantmatrix.o vector.o model.o utils.o meter.o fasttext.o src/main.cc -o fasttext\n",
            "fastText compiled in: /root/UnsupervisedMT/NMT/tools/fastText/fasttext\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "k1Rrb8IuxzfY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "1bf6207a-e14d-4603-80ed-75d8d4ac9a00"
      },
      "cell_type": "code",
      "source": [
        "#After uploading all.fr & all.en\n",
        "#cd UnsupervisedMT/NMT\n",
        "\n",
        "!./enfrB.sh"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./enfrB.sh: line 1: $'\\r': command not found\n",
            "./enfrB.sh: line 2: $'\\r': command not found\n",
            "./enfrB.sh: line 7: $'\\r': command not found\n",
            "./enfrB.sh: line 8: $'\\r': command not found\n",
            "./enfrB.sh: line 12: $'\\r': command not found\n",
            "./enfrB.sh: line 19: $'\\r': command not found\n",
            "./enfrB.sh: line 20: $'\\r': command not found\n",
            "./enfrB.sh: line 21: $'\\r': command not found\n",
            "./enfrB.sh: line 28: $'\\r': command not found\n",
            "./enfrB.sh: line 32: $'\\r': command not found\n",
            "./enfrB.sh: line 36: $'\\r': command not found\n",
            "./enfrB.sh: line 51: $'\\r': command not found\n",
            "./enfrB.sh: line 52: $'\\r': command not found\n",
            "./enfrB.sh: line 53: $'\\r': command not found\n",
            "./enfrB.sh: line 54: $'\\r': command not found\n",
            "./enfrB.sh: line 171: syntax error: unexpected end of file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d_gZe7uh2fVi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!cd ~\n",
        "#!cd UnsupervisedMT/NMT/data/para\n",
        "!mkdir dev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SJYEBdof2y69",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c736eda2-e255-4bf6-f27e-29c03f8bf796"
      },
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/UnsupervisedMT/NMT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pj-BfG5n26lD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7891
        },
        "outputId": "3ea6391e-2a8e-47f0-e5af-bd526f3372d3"
      },
      "cell_type": "code",
      "source": [
        "! printf 'y\\ny\\ny\\n' |python main.py --exp_name test --transformer True --n_enc_layers 4 --n_dec_layers 4 --share_enc 3 --share_dec 3 --share_lang_emb True --share_output_emb True --langs 'en,fr' --n_mono -1 --mono_dataset 'en:./data/mono/all.en.tok.60000.pth,,;fr:./data/mono/all.fr.tok.60000.pth,,' --para_dataset 'en-fr:,./data/para/dev/newstest2013-ref.XX.60000.pth,./data/para/dev/newstest2014-fren-src.XX.60000.pth' --mono_directions 'en,fr' --word_shuffle 3 --word_dropout 0.1 --word_blank 0.2 --pivo_directions 'fr-en-fr,en-fr-en' --pretrained_emb './data/mono/all.en-fr.60000.vec' --pretrained_out True --lambda_xe_mono '0:1,100000:0.1,300000:0' --lambda_xe_otfd 1 --otf_num_processes 30 --otf_sync_params_every 1000 --enc_optimizer adam,lr=0.0001 --epoch_size 500000 --stopping_criterion bleu_en_fr_valid,10\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO - 03/07/19 16:35:52 - 0:00:00 - ============ Initialized logger ============\n",
            "INFO - 03/07/19 16:35:52 - 0:00:00 - attention: True\n",
            "                                     attention_dropout: 0\n",
            "                                     back_dataset: {}\n",
            "                                     back_directions: []\n",
            "                                     batch_size: 32\n",
            "                                     beam_size: 0\n",
            "                                     clip_grad_norm: 5\n",
            "                                     command: python main.py --exp_name 'test' --transformer 'True' --n_enc_layers '4' --n_dec_layers '4' --share_enc '3' --share_dec '3' --share_lang_emb 'True' --share_output_emb 'True' --langs 'en,fr' --n_mono '-1' --mono_dataset 'en:./data/mono/all.en.tok.60000.pth,,;fr:./data/mono/all.fr.tok.60000.pth,,' --para_dataset 'en-fr:,./data/para/dev/newstest2013-ref.XX.60000.pth,./data/para/dev/newstest2014-fren-src.XX.60000.pth' --mono_directions 'en,fr' --word_shuffle '3' --word_dropout '0.1' --word_blank '0.2' --pivo_directions 'fr-en-fr,en-fr-en' --pretrained_emb './data/mono/all.en-fr.60000.vec' --pretrained_out 'True' --lambda_xe_mono '0:1,100000:0.1,300000:0' --lambda_xe_otfd '1' --otf_num_processes '30' --otf_sync_params_every '1000' --enc_optimizer 'adam,lr=0.0001' --epoch_size '500000' --stopping_criterion 'bleu_en_fr_valid,10' --exp_id \"7m9wgdq5u4\"\n",
            "                                     dec_optimizer: enc_optimizer\n",
            "                                     decoder_attention_heads: 8\n",
            "                                     decoder_normalize_before: False\n",
            "                                     dis_clip: 0\n",
            "                                     dis_dropout: 0\n",
            "                                     dis_hidden_dim: 128\n",
            "                                     dis_input_proj: True\n",
            "                                     dis_layers: 3\n",
            "                                     dis_optimizer: rmsprop,lr=0.0005\n",
            "                                     dis_smooth: 0\n",
            "                                     dropout: 0\n",
            "                                     dump_path: ./dumped/test/7m9wgdq5u4\n",
            "                                     emb_dim: 512\n",
            "                                     enc_optimizer: adam,lr=0.0001\n",
            "                                     encoder_attention_heads: 8\n",
            "                                     encoder_normalize_before: False\n",
            "                                     epoch_size: 500000\n",
            "                                     eval_only: False\n",
            "                                     exp_id: 7m9wgdq5u4\n",
            "                                     exp_name: test\n",
            "                                     freeze_dec_emb: False\n",
            "                                     freeze_enc_emb: False\n",
            "                                     group_by_size: True\n",
            "                                     hidden_dim: 512\n",
            "                                     id2lang: {0: 'en', 1: 'fr'}\n",
            "                                     label_smoothing: 0\n",
            "                                     lambda_dis: 0\n",
            "                                     lambda_lm: 0\n",
            "                                     lambda_xe_back: 0\n",
            "                                     lambda_xe_mono: 0:1,100000:0.1,300000:0\n",
            "                                     lambda_xe_otfa: 0\n",
            "                                     lambda_xe_otfd: 1\n",
            "                                     lambda_xe_para: 0\n",
            "                                     lang2id: {'en': 0, 'fr': 1}\n",
            "                                     langs: ['en', 'fr']\n",
            "                                     length_penalty: 1.0\n",
            "                                     lm_after: 0\n",
            "                                     lm_before: 0\n",
            "                                     lm_share_dec: 0\n",
            "                                     lm_share_emb: False\n",
            "                                     lm_share_enc: 0\n",
            "                                     lm_share_proj: False\n",
            "                                     lstm_proj: False\n",
            "                                     max_epoch: 100000\n",
            "                                     max_len: 175\n",
            "                                     max_vocab: -1\n",
            "                                     mono_dataset: {'en': ('./data/mono/all.en.tok.60000.pth', '', ''), 'fr': ('./data/mono/all.fr.tok.60000.pth', '', '')}\n",
            "                                     mono_directions: ['en', 'fr']\n",
            "                                     n_back: 0\n",
            "                                     n_dec_layers: 4\n",
            "                                     n_dis: 0\n",
            "                                     n_enc_layers: 4\n",
            "                                     n_langs: 2\n",
            "                                     n_mono: -1\n",
            "                                     n_para: 0\n",
            "                                     otf_backprop_temperature: -1\n",
            "                                     otf_num_processes: 30\n",
            "                                     otf_sample: -1\n",
            "                                     otf_sync_params_every: 1000\n",
            "                                     otf_update_dec: True\n",
            "                                     otf_update_enc: True\n",
            "                                     para_dataset: {('en', 'fr'): ('', './data/para/dev/newstest2013-ref.XX.60000.pth', './data/para/dev/newstest2014-fren-src.XX.60000.pth')}\n",
            "                                     para_directions: []\n",
            "                                     pivo_directions: [('fr', 'en', 'fr'), ('en', 'fr', 'en')]\n",
            "                                     pretrained_emb: ./data/mono/all.en-fr.60000.vec\n",
            "                                     pretrained_out: True\n",
            "                                     reload_dec: False\n",
            "                                     reload_dis: False\n",
            "                                     reload_enc: False\n",
            "                                     reload_model: \n",
            "                                     relu_dropout: 0\n",
            "                                     save_periodic: False\n",
            "                                     seed: -1\n",
            "                                     share_dec: 3\n",
            "                                     share_decpro_emb: False\n",
            "                                     share_enc: 3\n",
            "                                     share_encdec_emb: False\n",
            "                                     share_lang_emb: True\n",
            "                                     share_lstm_proj: False\n",
            "                                     share_output_emb: True\n",
            "                                     stopping_criterion: bleu_en_fr_valid,10\n",
            "                                     transformer: True\n",
            "                                     transformer_ffn_emb_dim: 2048\n",
            "                                     vocab: {}\n",
            "                                     vocab_min_count: 0\n",
            "                                     word_blank: 0.2\n",
            "                                     word_dropout: 0.1\n",
            "                                     word_shuffle: 3.0\n",
            "INFO - 03/07/19 16:35:52 - 0:00:00 - The experiment will be stored in ./dumped/test/7m9wgdq5u4\n",
            "                                     \n",
            "INFO - 03/07/19 16:35:52 - 0:00:00 - Running command: python main.py --exp_name 'test' --transformer 'True' --n_enc_layers '4' --n_dec_layers '4' --share_enc '3' --share_dec '3' --share_lang_emb 'True' --share_output_emb 'True' --langs 'en,fr' --n_mono '-1' --mono_dataset 'en:./data/mono/all.en.tok.60000.pth,,;fr:./data/mono/all.fr.tok.60000.pth,,' --para_dataset 'en-fr:,./data/para/dev/newstest2013-ref.XX.60000.pth,./data/para/dev/newstest2014-fren-src.XX.60000.pth' --mono_directions 'en,fr' --word_shuffle '3' --word_dropout '0.1' --word_blank '0.2' --pivo_directions 'fr-en-fr,en-fr-en' --pretrained_emb './data/mono/all.en-fr.60000.vec' --pretrained_out 'True' --lambda_xe_mono '0:1,100000:0.1,300000:0' --lambda_xe_otfd '1' --otf_num_processes '30' --otf_sync_params_every '1000' --enc_optimizer 'adam,lr=0.0001' --epoch_size '500000' --stopping_criterion 'bleu_en_fr_valid,10' --exp_id \"7m9wgdq5u4\"\n",
            "                                     \n",
            "INFO - 03/07/19 16:35:52 - 0:00:00 - ============ Parallel data (en - fr)\n",
            "INFO - 03/07/19 16:35:52 - 0:00:00 - Loading data from ./data/para/dev/newstest2013-ref.en.60000.pth ...\n",
            "INFO - 03/07/19 16:35:52 - 0:00:00 - 73148 words (59943 unique) in 3000 sentences. 15 unknown words (4 unique).\n",
            "INFO - 03/07/19 16:35:52 - 0:00:00 - Loading data from ./data/para/dev/newstest2013-ref.fr.60000.pth ...\n",
            "INFO - 03/07/19 16:35:52 - 0:00:00 - 119164 words (59943 unique) in 3000 sentences. 1257 unknown words (36 unique).\n",
            "INFO - 03/07/19 16:35:52 - 0:00:00 - Removed 0 empty sentences.\n",
            "INFO - 03/07/19 16:35:52 - 0:00:00 - Removed 3 too long sentences.\n",
            "INFO - 03/07/19 16:35:52 - 0:00:00 - Loading data from ./data/para/dev/newstest2014-fren-src.en.60000.pth ...\n",
            "INFO - 03/07/19 16:35:52 - 0:00:00 - 81276 words (59943 unique) in 3003 sentences. 24 unknown words (7 unique).\n",
            "INFO - 03/07/19 16:35:52 - 0:00:00 - Loading data from ./data/para/dev/newstest2014-fren-src.fr.60000.pth ...\n",
            "INFO - 03/07/19 16:35:52 - 0:00:00 - 135061 words (59943 unique) in 3003 sentences. 1235 unknown words (13 unique).\n",
            "INFO - 03/07/19 16:35:53 - 0:00:00 - Removed 0 empty sentences.\n",
            "\n",
            "\n",
            "INFO - 03/07/19 16:35:53 - 0:00:00 - ============ Monolingual data (en)\n",
            "INFO - 03/07/19 16:35:53 - 0:00:00 - Loading data from ./data/mono/all.en.tok.60000.pth ...\n",
            "INFO - 03/07/19 16:35:53 - 0:00:00 - 14397983 words (59943 unique) in 1862916 sentences. 0 unknown words (0 unique).\n",
            "INFO - 03/07/19 16:35:53 - 0:00:01 - Removed 47 empty sentences.\n",
            "INFO - 03/07/19 16:35:53 - 0:00:01 - Removed 356 too long sentences.\n",
            "INFO - 03/07/19 16:35:53 - 0:00:01 - ============ Monolingual data (fr)\n",
            "INFO - 03/07/19 16:35:53 - 0:00:01 - Loading data from ./data/mono/all.fr.tok.60000.pth ...\n",
            "INFO - 03/07/19 16:35:53 - 0:00:01 - 14829224 words (59943 unique) in 1567066 sentences. 0 unknown words (0 unique).\n",
            "INFO - 03/07/19 16:35:54 - 0:00:01 - Removed 32 empty sentences.\n",
            "INFO - 03/07/19 16:35:54 - 0:00:01 - Removed 401 too long sentences.\n",
            "\n",
            "INFO - 03/07/19 16:35:54 - 0:00:02 - ============ Data summary\n",
            "INFO - 03/07/19 16:35:54 - 0:00:02 - Parallel data      - valid -   en ->   fr:      2997\n",
            "INFO - 03/07/19 16:35:54 - 0:00:02 - Parallel data      -  test -   en ->   fr:      3003\n",
            "INFO - 03/07/19 16:35:54 - 0:00:02 - Monolingual data   - train -           en:   1862513\n",
            "INFO - 03/07/19 16:35:54 - 0:00:02 - Monolingual data   - valid -           en:         0\n",
            "INFO - 03/07/19 16:35:54 - 0:00:02 - Monolingual data   -  test -           en:         0\n",
            "INFO - 03/07/19 16:35:54 - 0:00:02 - Monolingual data   - train -           fr:   1566633\n",
            "INFO - 03/07/19 16:35:54 - 0:00:02 - Monolingual data   - valid -           fr:         0\n",
            "INFO - 03/07/19 16:35:54 - 0:00:02 - Monolingual data   -  test -           fr:         0\n",
            "\n",
            "INFO - 03/07/19 16:35:54 - 0:00:02 - ============ Building transformer attention model - Encoder ...\n",
            "INFO - 03/07/19 16:35:54 - 0:00:02 - Sharing encoder input embeddings\n",
            "INFO - 03/07/19 16:35:55 - 0:00:02 - Sharing encoder transformer parameters for layer 1\n",
            "INFO - 03/07/19 16:35:55 - 0:00:02 - Sharing encoder transformer parameters for layer 2\n",
            "INFO - 03/07/19 16:35:55 - 0:00:03 - Sharing encoder transformer parameters for layer 3\n",
            "\n",
            "INFO - 03/07/19 16:35:55 - 0:00:03 - ============ Building transformer attention model - Decoder ...\n",
            "INFO - 03/07/19 16:35:55 - 0:00:03 - Sharing decoder input embeddings\n",
            "INFO - 03/07/19 16:35:56 - 0:00:03 - Sharing decoder transformer parameters for layer 0\n",
            "INFO - 03/07/19 16:35:56 - 0:00:03 - Sharing decoder transformer parameters for layer 1\n",
            "INFO - 03/07/19 16:35:56 - 0:00:03 - Sharing decoder transformer parameters for layer 2\n",
            "INFO - 03/07/19 16:35:56 - 0:00:04 - Sharing decoder projection matrices\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "INFO - 03/07/19 16:36:02 - 0:00:10 - Reloading embeddings from ./data/mono/all.en-fr.60000.vec ...\n",
            "INFO - 03/07/19 16:36:08 - 0:00:16 - Reloaded 59930 embeddings.\n",
            "INFO - 03/07/19 16:36:14 - 0:00:22 - Initialized 59930 / 59943 word embeddings for \"en\" (including 0 after lowercasing).\n",
            "INFO - 03/07/19 16:36:14 - 0:00:22 - Initialized 59930 / 59943 word embeddings for \"fr\" (including 0 after lowercasing).\n",
            "INFO - 03/07/19 16:36:14 - 0:00:22 - ============ Model summary\n",
            "INFO - 03/07/19 16:36:14 - 0:00:22 - Number of enc+dec parameters: 128914471\n",
            "INFO - 03/07/19 16:36:14 - 0:00:22 - Encoder: TransformerEncoder(\n",
            "                                       (embeddings): ModuleList(\n",
            "                                         (0): Embedding(59943, 512, padding_idx=2)\n",
            "                                         (1): Embedding(59943, 512, padding_idx=2)\n",
            "                                       )\n",
            "                                       (embed_positions): SinusoidalPositionalEmbedding()\n",
            "                                       (layers): ModuleList(\n",
            "                                         (0): ModuleList(\n",
            "                                           (0): TransformerEncoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                           (1): TransformerEncoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                         )\n",
            "                                         (1): ModuleList(\n",
            "                                           (0): TransformerEncoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                           (1): TransformerEncoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                         )\n",
            "                                         (2): ModuleList(\n",
            "                                           (0): TransformerEncoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                           (1): TransformerEncoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                         )\n",
            "                                         (3): ModuleList(\n",
            "                                           (0): TransformerEncoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                           (1): TransformerEncoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                         )\n",
            "                                       )\n",
            "                                     )\n",
            "INFO - 03/07/19 16:36:14 - 0:00:22 - Decoder: TransformerDecoder(\n",
            "                                       (embeddings): ModuleList(\n",
            "                                         (0): Embedding(59943, 512, padding_idx=2)\n",
            "                                         (1): Embedding(59943, 512, padding_idx=2)\n",
            "                                       )\n",
            "                                       (embed_positions): SinusoidalPositionalEmbedding()\n",
            "                                       (layers): ModuleList(\n",
            "                                         (0): ModuleList(\n",
            "                                           (0): TransformerDecoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (encoder_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                               (2): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                           (1): TransformerDecoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (encoder_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                               (2): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                         )\n",
            "                                         (1): ModuleList(\n",
            "                                           (0): TransformerDecoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (encoder_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                               (2): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                           (1): TransformerDecoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (encoder_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                               (2): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                         )\n",
            "                                         (2): ModuleList(\n",
            "                                           (0): TransformerDecoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (encoder_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                               (2): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                           (1): TransformerDecoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (encoder_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                               (2): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                         )\n",
            "                                         (3): ModuleList(\n",
            "                                           (0): TransformerDecoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (encoder_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                               (2): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                           (1): TransformerDecoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (encoder_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                               (2): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                         )\n",
            "                                       )\n",
            "                                       (proj): ModuleList(\n",
            "                                         (0): Linear(in_features=512, out_features=59943, bias=True)\n",
            "                                         (1): Linear(in_features=512, out_features=59943, bias=True)\n",
            "                                       )\n",
            "                                       (loss_fn): ModuleList(\n",
            "                                         (0): CrossEntropyLoss()\n",
            "                                         (1): CrossEntropyLoss()\n",
            "                                       )\n",
            "                                     )\n",
            "INFO - 03/07/19 16:36:14 - 0:00:22 - Discriminator: None\n",
            "INFO - 03/07/19 16:36:14 - 0:00:22 - LM: None\n",
            "\n",
            "INFO - 03/07/19 16:36:21 - 0:00:29 - Starting subprocesses for OTF generation ...\n",
            "INFO - 03/07/19 16:36:21 - 0:00:29 - Stopping criterion: bleu_en_fr_valid,10\n",
            "INFO - 03/07/19 16:36:23 - 0:00:31 - Test: Parameters are shared correctly.\n",
            "INFO - 03/07/19 16:37:00 - 0:01:08 - ====================== Starting epoch 0 ... ======================\n",
            "INFO - 03/07/19 16:37:00 - 0:01:08 - Creating new training encdec,en iterator ...\n",
            "INFO - 03/07/19 16:37:20 - 0:01:27 - Creating new training encdec,fr iterator ...\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 356, in <module>\n",
            "    main(params)\n",
            "  File \"main.py\", line 313, in main\n",
            "    trainer.otf_sync_params()\n",
            "  File \"/root/UnsupervisedMT/NMT/src/trainer.py\", line 537, in otf_sync_params\n",
            "    decoder_params=decoder_params)\n",
            "  File \"/root/UnsupervisedMT/NMT/src/multiprocessing_event_loop.py\", line 82, in call_async\n",
            "    self.input_pipes[rank].send((result_type, action, kwargs))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 206, in send\n",
            "    self._send_bytes(_ForkingPickler.dumps(obj))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}